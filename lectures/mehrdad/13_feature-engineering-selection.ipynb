{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](../img/330-banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Lecture 13: Feature engineering and feature selection \n",
    "\n",
    "UBC 2024 Summer\n",
    "\n",
    "Instructor: Mehrdad Oveisi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import pandas as pd\n",
    "from sklearn.compose import (\n",
    "    ColumnTransformer,\n",
    "    TransformedTargetRegressor,\n",
    "    make_column_transformer,\n",
    ")\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, RidgeCV\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Learning outcomes \n",
    "\n",
    "From this lecture, students are expected to be able to:\n",
    "\n",
    "- Explain what feature engineering is and the importance of feature engineering in building machine learning models.  \n",
    "- Carry out preliminary feature engineering on numeric and text data. \n",
    "- Explain the general concept of feature selection. \n",
    "- Discuss and compare different feature selection methods at a high level. \n",
    "- Use `sklearn`'s implementation of model-based selection and recursive feature elimination (`RFE`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feature engineering: Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ❓❓ Questions for you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select the most accurate option below.**\n",
    "\n",
    "Suppose you are working on a machine learning project. If you have to prioritize one of the following in your project which of the following would it be? \n",
    "\n",
    "- (A) The quality and size of the data \n",
    "- (B) Most recent deep neural network model \n",
    "- (C) Most recent optimization algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion question**\n",
    "- Suppose we want to predict whether a flight will arrive on time or be delayed. We have a dataset with the following information about flights:\n",
    "    - Departure Time\n",
    "    - Expected Duration of Flight (in minutes)\n",
    "\n",
    "Upon analyzing the data, you notice a pattern: flights tend to be delayed more often during the evening rush hours. What feature could be valuable to add for this prediction task?\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Garbage in, garbage out.\n",
    "\n",
    "- Model building is interesting. But in your machine learning projects, you'll be spending more than half of your time on data preparation, feature engineering, and transformations.\n",
    "- The _quality_ of the data is important. Your model is only as good as your data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is feature engineering?  \n",
    "\n",
    "- Better features: more flexibility, higher score, we can get by with simple and more interpretable models. \n",
    "- If your features, i.e., representation is bad, whatever fancier model you build is not going to help.\n",
    "\n",
    "<blockquote>\n",
    "<b>Feature engineering</b> is the process of <b>transforming raw data into features</b> that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data.<br> \n",
    "- Jason Brownlee    \n",
    "</blockquote>    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Some quotes on feature engineering \n",
    "\n",
    "A quote by Pedro Domingos [A Few Useful Things to Know About Machine Learning](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf)\n",
    "\n",
    "<blockquote>\n",
    "... At the end of the day, some machine learning projects succeed and some fail. What makes the difference? Easily the most important factor is the features used. \n",
    "</blockquote>\n",
    "\n",
    "\n",
    "A quote by Andrew Ng, [Machine Learning and AI via Brain simulations](https://ai.stanford.edu/~ang/slides/DeepLearning-Mar2013.pptx)\n",
    "\n",
    "<blockquote>\n",
    "Coming up with features is difficult, time-consuming, requires expert knowledge. \"Applied machine learning\" is basically feature engineering.\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Better features usually help more than a better model.\n",
    "- Good features would ideally:\n",
    "    - capture **most important aspects** of the problem\n",
    "    - allow learning with **few examples** \n",
    "    - **generalize** to new scenarios.\n",
    "- There is a trade-off between **simple** and **expressive** features:\n",
    "    - With **simple features** overfitting risk is low, but **scores might be low**.\n",
    "    - With **complicated features** scores can be high, but so is **overfitting risk**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### The best features may be dependent on the model you use.\n",
    "\n",
    "- Examples:\n",
    "    - For counting-based methods like decision trees, separate relevant **groups of variable values**\n",
    "        - Discretization\n",
    "           - Partitioning and converting continuous attributes into discrete intervals\n",
    "           - Enables using continuous features for algorithms requiring discrete features\n",
    "    - For distance-based methods like kNN, we want different class labels to be \"far\".\n",
    "        - Standardization \n",
    "           - Avoid dominance of wide-ranging features over other features with smaller ranges\n",
    "    - For regression-based methods like linear regression, we want targets to have a linear dependency on features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Domain-specific transformations\n",
    "\n",
    "In some domains there are natural transformations to do:\n",
    "- Spectrograms (sound data)\n",
    "- Wavelets (image data)\n",
    "- Convolutions \n",
    "\n",
    "![](../img/spectogram.png)\n",
    "\n",
    "<!-- <img src=\"img/spectogram.png\" width=\"800\" height=\"800\"> -->\n",
    "\n",
    "[Source](https://en.wikipedia.org/wiki/Spectrogram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this lecture, I'll show you an example of feature engineering on text data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature interactions and feature crosses\n",
    "\n",
    "- A **feature cross** is a synthetic feature formed by multiplying or crossing two or more features. \n",
    "- Example: \n",
    "Is the following dataset (XOR function) linearly separable?  \n",
    "\n",
    "| $$x_1$$ | $$x_2$$ | target|\n",
    "|---------|---------|---------|\n",
    "| 1 | 1  | 0|\n",
    "| -1 | 1  | 1|\n",
    "| 1 | -1  | 1|\n",
    "| -1 | -1  | 0|    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msb\u001b[39;00m\n\u001b[0;32m      2\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\n\u001b[0;32m      3\u001b[0m     [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m      4\u001b[0m     [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m      5\u001b[0m     [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m      6\u001b[0m     [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m      7\u001b[0m ])\n\u001b[0;32m      8\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import seaborn as sb\n",
    "X = np.array([\n",
    "    [-1, -1],\n",
    "    [1, -1],\n",
    "    [-1, 1],\n",
    "    [1, 1]\n",
    "])\n",
    "y = np.array([1, 0, 0, 1])\n",
    "df = pd.DataFrame(np.column_stack([X, y]), columns=[\"X1\", \"X2\", \"target\"])\n",
    "plt.figure(figsize=(4, 4))\n",
    "sb.scatterplot(data=df, x=\"X1\", y=\"X2\", style=\"target\", s=200, legend=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For XOR like problems, if we create a feature cross $x1x2$, the data becomes linearly separable. \n",
    "\n",
    "| $$x_1$$ | $$x_2$$ | $$x_1x_2$$ | target|\n",
    "|---------|---------|---------|---------|\n",
    "| 1 | 1  | 1 | 0|\n",
    "| -1 | 1  | -1 | 1|\n",
    "| 1 | -1  | -1 | 1|\n",
    "| -1 | -1  | 1 | 0|    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX1X2\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX1\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m*\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX2\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      2\u001b[0m df\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df[\"X1X2\"] = df[\"X1\"] * df[\"X2\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m----> 2\u001b[0m \u001b[43msb\u001b[49m\u001b[38;5;241m.\u001b[39mscatterplot(data\u001b[38;5;241m=\u001b[39mdf, x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX2\u001b[39m\u001b[38;5;124m\"\u001b[39m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX1X2\u001b[39m\u001b[38;5;124m\"\u001b[39m, style\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m, s\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, legend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m);\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sb' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 400x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "sb.scatterplot(data=df, x=\"X2\", y=\"X1X2\", style=\"target\", s=200, legend=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an example with more data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.76405235,  0.40015721],\n",
       "       [ 0.97873798,  2.2408932 ],\n",
       "       [ 1.86755799, -0.97727788],\n",
       "       [ 0.95008842, -0.15135721]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx, yy = np.meshgrid(np.linspace(-3, 3, 50), np.linspace(-3, 3, 50))\n",
    "rng = np.random.RandomState(0)\n",
    "rng.randn(4, 2)  # example output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_xor = rng.randn(200, 2)\n",
    "y_xor = np.logical_xor(X_xor[:, 0] > 0, X_xor[:, 1] > 0)\n",
    "# Interaction term\n",
    "Z = X_xor[:, 0] * X_xor[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>Z</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.103219</td>\n",
       "      <td>0.410599</td>\n",
       "      <td>-0.042382</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.144044</td>\n",
       "      <td>1.454274</td>\n",
       "      <td>0.209479</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.761038</td>\n",
       "      <td>0.121675</td>\n",
       "      <td>0.092599</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.443863</td>\n",
       "      <td>0.333674</td>\n",
       "      <td>0.148106</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.494079</td>\n",
       "      <td>-0.205158</td>\n",
       "      <td>-0.306523</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          X         Y         Z  Class\n",
       "0 -0.103219  0.410599 -0.042382   True\n",
       "1  0.144044  1.454274  0.209479  False\n",
       "2  0.761038  0.121675  0.092599  False\n",
       "3  0.443863  0.333674  0.148106  False\n",
       "4  1.494079 -0.205158 -0.306523   True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'X': X_xor[:, 0], 'Y': X_xor[:, 1], 'Z': Z, 'Class': y_xor})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(df[df['Class'] == True]['X'], df[df['Class'] == True]['Y'], c='blue', label='Class 0', s=50)\n",
    "plt.scatter(df[df['Class'] == False]['X'], df[df['Class'] == False]['Y'], c='red', label='Class 0', s=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create an interactive 3D scatter plot using plotly\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpress\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpx\u001b[39;00m\n\u001b[0;32m      3\u001b[0m fig \u001b[38;5;241m=\u001b[39m px\u001b[38;5;241m.\u001b[39mscatter_3d(df, x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m'\u001b[39m, z\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mZ\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClass\u001b[39m\u001b[38;5;124m'\u001b[39m, color_continuous_scale\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      4\u001b[0m fig\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "# Create an interactive 3D scatter plot using plotly\n",
    "import plotly.express as px\n",
    "fig = px.scatter_3d(df, x='X', y='Y', z='Z', color='Class', color_continuous_scale=['blue', 'red'])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogisticRegression().fit(X_xor, y_xor).score(X_xor, y_xor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.985"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "pipe_xor = make_pipeline(\n",
    "    PolynomialFeatures(interaction_only=True, include_bias=False), LogisticRegression()\n",
    ")\n",
    "pipe_xor.fit(X_xor, y_xor)\n",
    "pipe_xor.score(X_xor, y_xor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = (\n",
    "    pipe_xor.named_steps[\"polynomialfeatures\"].get_feature_names_out().tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# transformed = pipe_xor.named_steps[\"polynomialfeatures\"].transform(X_xor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>x0</th>\n",
       "      <td>-0.101267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x1</th>\n",
       "      <td>0.134765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x0 x1</th>\n",
       "      <td>-5.109436</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Feature coefficient\n",
       "x0               -0.101267\n",
       "x1                0.134765\n",
       "x0 x1            -5.109436"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    pipe_xor.named_steps[\"logisticregression\"].coef_.transpose(),\n",
    "    index=feature_names,\n",
    "    columns=[\"Feature coefficient\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interaction feature has the biggest coefficient! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature crosses for one-hot encoded features\n",
    "- You can think of **feature crosses of one-hot-features as logical conjunctions**\n",
    "- Suppose you want to predict whether you will **find parking or not** based on two features: \n",
    "    - **area** (possible categories: UBC campus and Rogers Arena)\n",
    "    - **time** of the day (possible categories: 9am and 7pm)\n",
    "- A **feature cross** in this case would create **four new features**: \n",
    "    - UBC campus and 9am\n",
    "    - UBC campus and 7pm\n",
    "    - Rogers Arena and 9am\n",
    "    - Rogers Arena and 7pm. \n",
    "- The features UBC campus and 9am on their own are not that informative but the newly created feature UBC campus and 9am or Rogers Arena and 7pm would be quite informative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Coming up with the right combination of features **requires** some **domain knowledge** or careful **examination of the data**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo of feature engineering with numeric features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remember the [California housing dataset](https://www.kaggle.com/datasets/camnugent/california-housing-prices) we used earlier in the course? \n",
    "- The prediction task is predicting `median_house_value` for a given property. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/california_housing.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m housing_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/california_housing.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m housing_df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cpsc330\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cpsc330\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cpsc330\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cpsc330\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cpsc330\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/california_housing.csv'"
     ]
    }
   ],
   "source": [
    "housing_df = pd.read_csv(\"../data/california_housing.csv\")\n",
    "housing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we decide to train `ridge` model on this dataset. \n",
    "\n",
    "- What would happen if you train a model without applying any transformation on the categorical features `ocean_proximity`?\n",
    "    - Error!! A linear model requires all features in a numeric form.  \n",
    "- What would happen if we apply OHE on `ocean_proximity` but we do not scale the features?\n",
    "    - No syntax error. But the model results are likely to be poor. \n",
    "- Do we need to apply any other transformations on this data?     \n",
    "\n",
    "In this section, we will look into some common ways to do **feature engineering** for **numeric** or **categorical** features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'housing_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_df, test_df \u001b[38;5;241m=\u001b[39m train_test_split(\u001b[43mhousing_df\u001b[49m, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m123\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'housing_df' is not defined"
     ]
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(housing_df, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have total rooms and the number of households in the neighbourhood. How about **creating `rooms_per_household` feature** using this information? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_df \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_df\u001b[49m\u001b[38;5;241m.\u001b[39massign(\n\u001b[0;32m      2\u001b[0m     rooms_per_household\u001b[38;5;241m=\u001b[39mtrain_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_rooms\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m train_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhouseholds\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      3\u001b[0m )\n\u001b[0;32m      4\u001b[0m test_df \u001b[38;5;241m=\u001b[39m test_df\u001b[38;5;241m.\u001b[39massign(\n\u001b[0;32m      5\u001b[0m     rooms_per_household\u001b[38;5;241m=\u001b[39mtest_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_rooms\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m test_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhouseholds\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      6\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "train_df = train_df.assign(\n",
    "    rooms_per_household=train_df[\"total_rooms\"] / train_df[\"households\"]\n",
    ")\n",
    "test_df = test_df.assign(\n",
    "    rooms_per_household=test_df[\"total_rooms\"] / test_df[\"households\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_df\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's start simple**. Imagine that we only three features: `longitude`, `latitude`, and our newly created `rooms_per_household` feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_train_housing \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_df\u001b[49m[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrooms_per_household\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m      2\u001b[0m y_train_housing \u001b[38;5;241m=\u001b[39m train_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedian_house_value\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "X_train_housing = train_df[[\"latitude\", \"longitude\", \"rooms_per_household\"]]\n",
    "y_train_housing = train_df[\"median_house_value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "numeric_feats = [\"latitude\", \"longitude\", \"rooms_per_household\"]\n",
    "\n",
    "preprocessor1 = make_column_transformer(\n",
    "    (make_pipeline(SimpleImputer(), StandardScaler()), numeric_feats)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_housing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m lr_1 \u001b[38;5;241m=\u001b[39m make_pipeline(preprocessor1, Ridge())\n\u001b[0;32m      2\u001b[0m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[1;32m----> 3\u001b[0m     cross_validate(lr_1, \u001b[43mX_train_housing\u001b[49m, y_train_housing, return_train_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_housing' is not defined"
     ]
    }
   ],
   "source": [
    "lr_1 = make_pipeline(preprocessor1, Ridge())\n",
    "pd.DataFrame(\n",
    "    cross_validate(lr_1, X_train_housing, y_train_housing, return_train_score=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The scores are not great. \n",
    "- Let's look at the distribution of the longitude and latitude features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m4\u001b[39m), dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39mhist(\u001b[43mtrain_df\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m\"\u001b[39m], bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDistribution of longitude feature\u001b[39m\u001b[38;5;124m\"\u001b[39m);\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 480x320 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6, 4), dpi=80)\n",
    "plt.hist(train_df[\"longitude\"], bins=50)\n",
    "plt.title(\"Distribution of longitude feature\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m4\u001b[39m), dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39mhist(\u001b[43mtrain_df\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m\"\u001b[39m], bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDistribution of latitude feature\u001b[39m\u001b[38;5;124m\"\u001b[39m);\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 480x320 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6, 4), dpi=80)\n",
    "plt.hist(train_df[\"latitude\"], bins=50)\n",
    "plt.title(\"Distribution of latitude feature\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Suppose you are planning to build a linear model for housing price prediction. \n",
    "- If we think longitude is a good feature for prediction, does it makes sense to use the floating point representation of this feature that's given to us?\n",
    "- Remember that **linear models can capture only linear relationships**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How about **discretizing latitude and longitude** features and putting them into buckets?\n",
    "- This process of transforming numeric features into categorical features is **called bucketing or binning**. \n",
    "- In `sklearn` you can do this using `KBinsDiscretizer` transformer. \n",
    "- Let's examine whether we get better results with binning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "discretization_feats = [\"latitude\", \"longitude\"]\n",
    "numeric_feats = [\"rooms_per_household\"]\n",
    "\n",
    "preprocessor2 = make_column_transformer(\n",
    "    (KBinsDiscretizer(n_bins=20, encode=\"onehot\"), discretization_feats),\n",
    "    (make_pipeline(SimpleImputer(), StandardScaler()), numeric_feats),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_housing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m lr_2 \u001b[38;5;241m=\u001b[39m make_pipeline(preprocessor2, Ridge())\n\u001b[0;32m      2\u001b[0m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[1;32m----> 3\u001b[0m     cross_validate(lr_2, \u001b[43mX_train_housing\u001b[49m, y_train_housing, return_train_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_housing' is not defined"
     ]
    }
   ],
   "source": [
    "lr_2 = make_pipeline(preprocessor2, Ridge())\n",
    "pd.DataFrame(\n",
    "    cross_validate(lr_2, X_train_housing, y_train_housing, return_train_score=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are better with binned features. Let's examine how do these binned features look like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_housing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m lr_2\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train_housing\u001b[49m, y_train_housing)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_housing' is not defined"
     ]
    }
   ],
   "source": [
    "lr_2.fit(X_train_housing, y_train_housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_housing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[1;32m----> 2\u001b[0m     preprocessor2\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mX_train_housing\u001b[49m)\u001b[38;5;241m.\u001b[39mtodense(),\n\u001b[0;32m      3\u001b[0m     columns\u001b[38;5;241m=\u001b[39mpreprocessor2\u001b[38;5;241m.\u001b[39mget_feature_names_out(),\n\u001b[0;32m      4\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_housing' is not defined"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    preprocessor2.fit_transform(X_train_housing).todense(),\n",
    "    columns=preprocessor2.get_feature_names_out(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about discretizing all three features? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "discretization_feats = [\"latitude\", \"longitude\", \"rooms_per_household\"]\n",
    "\n",
    "preprocessor3 = make_column_transformer(\n",
    "    (KBinsDiscretizer(n_bins=20, encode=\"onehot\"), discretization_feats),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_housing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m lr_3 \u001b[38;5;241m=\u001b[39m make_pipeline(preprocessor3, Ridge())\n\u001b[0;32m      2\u001b[0m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[1;32m----> 3\u001b[0m     cross_validate(lr_3, \u001b[43mX_train_housing\u001b[49m, y_train_housing, return_train_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_housing' is not defined"
     ]
    }
   ],
   "source": [
    "lr_3 = make_pipeline(preprocessor3, Ridge())\n",
    "pd.DataFrame(\n",
    "    cross_validate(lr_3, X_train_housing, y_train_housing, return_train_score=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The results have improved further!! \n",
    "- Let's examine the coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_housing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m lr_3\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train_housing\u001b[49m, y_train_housing)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_housing' is not defined"
     ]
    }
   ],
   "source": [
    "lr_3.fit(X_train_housing, y_train_housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ColumnTransformer' object has no attribute 'transformers_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m feature_names \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m      2\u001b[0m     \u001b[43mlr_3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnamed_steps\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumntransformer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m----> 3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnamed_transformers_\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkbinsdiscretizer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mget_feature_names_out()\n\u001b[0;32m      5\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cpsc330\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:520\u001b[0m, in \u001b[0;36mColumnTransformer.named_transformers_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Access the fitted transformer by name.\u001b[39;00m\n\u001b[0;32m    514\u001b[0m \n\u001b[0;32m    515\u001b[0m \u001b[38;5;124;03mRead-only attribute to access any transformer by given name.\u001b[39;00m\n\u001b[0;32m    516\u001b[0m \u001b[38;5;124;03mKeys are transformer names and values are the fitted transformer\u001b[39;00m\n\u001b[0;32m    517\u001b[0m \u001b[38;5;124;03mobjects.\u001b[39;00m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;66;03m# Use Bunch object to improve autocomplete\u001b[39;00m\n\u001b[1;32m--> 520\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Bunch(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{name: trans \u001b[38;5;28;01mfor\u001b[39;00m name, trans, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformers_\u001b[49m})\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ColumnTransformer' object has no attribute 'transformers_'"
     ]
    }
   ],
   "source": [
    "feature_names = (\n",
    "    lr_3.named_steps[\"columntransformer\"]\n",
    "    .named_transformers_[\"kbinsdiscretizer\"]\n",
    "    .get_feature_names_out()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Ridge' object has no attribute 'coef_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mlr_3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnamed_steps\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mridge\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoef_\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Ridge' object has no attribute 'coef_'"
     ]
    }
   ],
   "source": [
    "lr_3.named_steps[\"ridge\"].coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Ridge' object has no attribute 'coef_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m coefs_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mlr_3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnamed_steps\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mridge\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoef_\u001b[49m\u001b[38;5;241m.\u001b[39mtranspose(),\n\u001b[0;32m      3\u001b[0m     index\u001b[38;5;241m=\u001b[39mfeature_names,\n\u001b[0;32m      4\u001b[0m     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoefficient\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      5\u001b[0m )\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoefficient\u001b[39m\u001b[38;5;124m\"\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m coefs_df\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Ridge' object has no attribute 'coef_'"
     ]
    }
   ],
   "source": [
    "coefs_df = pd.DataFrame(\n",
    "    lr_3.named_steps[\"ridge\"].coef_.transpose(),\n",
    "    index=feature_names,\n",
    "    columns=[\"coefficient\"],\n",
    ").sort_values(\"coefficient\", ascending=False)\n",
    "coefs_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Does it make sense to take feature crosses in this context?\n",
    "- What information would they encode? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo of feature engineering for text data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using [Covid tweets](https://www.kaggle.com/code/kerneler/starter-covid-19-nlp-text-d3a3baa6-e/data) dataset for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/Corona_NLP_test.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/Corona_NLP_test.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentiment\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cpsc330\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cpsc330\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cpsc330\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cpsc330\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cpsc330\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/Corona_NLP_test.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/Corona_NLP_test.csv')\n",
    "df['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>Z</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.066517</td>\n",
       "      <td>0.302472</td>\n",
       "      <td>0.020120</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0.693773</td>\n",
       "      <td>-0.159573</td>\n",
       "      <td>-0.110708</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>1.494485</td>\n",
       "      <td>-2.069985</td>\n",
       "      <td>-3.093561</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.402342</td>\n",
       "      <td>-0.684810</td>\n",
       "      <td>-0.275528</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.028182</td>\n",
       "      <td>0.428332</td>\n",
       "      <td>-0.012071</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-1.706270</td>\n",
       "      <td>1.950775</td>\n",
       "      <td>-3.328550</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.640132</td>\n",
       "      <td>-1.616956</td>\n",
       "      <td>-1.035065</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>-1.491258</td>\n",
       "      <td>0.439392</td>\n",
       "      <td>-0.655246</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>-1.188859</td>\n",
       "      <td>-0.506816</td>\n",
       "      <td>0.602533</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0.156507</td>\n",
       "      <td>0.232181</td>\n",
       "      <td>0.036338</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            X         Y         Z  Class\n",
       "24   0.066517  0.302472  0.020120  False\n",
       "150  0.693773 -0.159573 -0.110708   True\n",
       "114  1.494485 -2.069985 -3.093561   True\n",
       "33   0.402342 -0.684810 -0.275528   True\n",
       "23  -0.028182  0.428332 -0.012071   True\n",
       "..        ...       ...       ...    ...\n",
       "17  -1.706270  1.950775 -3.328550   True\n",
       "98   0.640132 -1.616956 -1.035065   True\n",
       "66  -1.491258  0.439392 -0.655246   True\n",
       "126 -1.188859 -0.506816  0.602533  False\n",
       "109  0.156507  0.232181  0.036338  False\n",
       "\n",
       "[160 rows x 4 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['X', 'Y', 'Z', 'Class'], dtype='object')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Location'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\cpsc330\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Location'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLocation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalue_counts()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cpsc330\\lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cpsc330\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Location'"
     ]
    }
   ],
   "source": [
    "train_df['Location'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train_df[['OriginalTweet', 'Location']], train_df['Sentiment']\n",
    "X_test, y_test = test_df[['OriginalTweet', 'Location']], test_df['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_metrics = 'accuracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_std_cross_val_scores(model, X_train, y_train, **kwargs):\n",
    "    \"\"\"\n",
    "    Returns mean and std of cross validation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model :\n",
    "        scikit-learn model\n",
    "    X_train : numpy array or pandas DataFrame\n",
    "        X in the training data\n",
    "    y_train :\n",
    "        y in the training data\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "        pandas Series with mean scores from cross_validation\n",
    "    \"\"\"\n",
    "\n",
    "    scores = cross_validate(model, X_train, y_train, **kwargs)\n",
    "\n",
    "    mean_scores = pd.DataFrame(scores).mean()\n",
    "    std_scores = pd.DataFrame(scores).std()\n",
    "    out_col = []\n",
    "\n",
    "    for i in range(len(mean_scores)):\n",
    "        out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores.iloc[i], std_scores.iloc[i])))\n",
    "\n",
    "    return pd.Series(data=out_col, index=mean_scores.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mean_std_cross_val_scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m dummy \u001b[38;5;241m=\u001b[39m DummyClassifier()\n\u001b[1;32m----> 2\u001b[0m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdummy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mmean_std_cross_val_scores\u001b[49m(\n\u001b[0;32m      3\u001b[0m     dummy, X_train, y_train, return_train_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, scoring\u001b[38;5;241m=\u001b[39mscoring_metrics\n\u001b[0;32m      4\u001b[0m )\n\u001b[0;32m      5\u001b[0m pd\u001b[38;5;241m.\u001b[39mDataFrame(results)\u001b[38;5;241m.\u001b[39mT\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mean_std_cross_val_scores' is not defined"
     ]
    }
   ],
   "source": [
    "dummy = DummyClassifier()\n",
    "results[\"dummy\"] = mean_std_cross_val_scores(\n",
    "    dummy, X_train, y_train, return_train_score=True, scoring=scoring_metrics\n",
    ")\n",
    "pd.DataFrame(results).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-words model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mean_std_cross_val_scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n\u001b[0;32m      2\u001b[0m pipe \u001b[38;5;241m=\u001b[39m make_pipeline(CountVectorizer(stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m), \n\u001b[0;32m      3\u001b[0m                      LogisticRegression(max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m----> 4\u001b[0m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogistic regression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mmean_std_cross_val_scores\u001b[49m(\n\u001b[0;32m      5\u001b[0m     pipe, X_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOriginalTweet\u001b[39m\u001b[38;5;124m'\u001b[39m], y_train, return_train_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, scoring\u001b[38;5;241m=\u001b[39mscoring_metrics\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      7\u001b[0m pd\u001b[38;5;241m.\u001b[39mDataFrame(results)\u001b[38;5;241m.\u001b[39mT\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mean_std_cross_val_scores' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "pipe = make_pipeline(CountVectorizer(stop_words='english'), \n",
    "                     LogisticRegression(max_iter=1000))\n",
    "results[\"logistic regression\"] = mean_std_cross_val_scores(\n",
    "    pipe, X_train['OriginalTweet'], y_train, return_train_score=True, scoring=scoring_metrics\n",
    ")\n",
    "pd.DataFrame(results).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is it possible to further improve the scores?\n",
    "\n",
    "- How about adding new features based on our intuitions? Let's extract our own features that might be useful for this prediction task. In other words, let's carry out **feature engineering**. \n",
    "\n",
    "- The code below adds some very basic length-related and sentiment features. We will be using a popular library called `nltk` for this exercise. If you have successfully created the course `conda` environment on your machine, you should already have this package in the environment.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- How do we extract interesting information from text?\n",
    "- We use **pre-trained models**! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- A couple of popular libraries which include such pre-trained models. \n",
    "- `nltk`\n",
    "```\n",
    "conda install -n cpsc330 -c anaconda nltk \n",
    "```        \n",
    "- spaCy\n",
    "```\n",
    "conda install -n cpsc330 -c conda-forge spacy\n",
    "```\n",
    "\n",
    "For emoji support: \n",
    "```\n",
    "pip install spacymoji\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You also need to download the language model which contains all the pre-trained models. For that run the following in your course `conda` environment or here.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# !python -m spacy download en_core_web_md\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# !python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      3\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpunkt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mnltk\u001b[49m\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvader_lexicon\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpunkt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msentiment\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentimentIntensityAnalyzer\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "nltk.download(\"vader_lexicon\")\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCPSC 330 students are smart and funny.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43msid\u001b[49m\u001b[38;5;241m.\u001b[39mpolarity_scores(s))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sid' is not defined"
     ]
    }
   ],
   "source": [
    "s = \"CPSC 330 students are smart and funny.\"\n",
    "print(sid.polarity_scores(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCPSC 330 students are tired because of all the hard work they have been doing.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43msid\u001b[49m\u001b[38;5;241m.\u001b[39mpolarity_scores(s))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sid' is not defined"
     ]
    }
   ],
   "source": [
    "s = \"CPSC 330 students are tired because of all the hard work they have been doing.\"\n",
    "print(sid.polarity_scores(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### [spaCy](https://spacy.io/) \n",
    "\n",
    "A useful package for text processing and feature extraction\n",
    "- Active development: https://github.com/explosion/spaCy\n",
    "- Interactive lessons by Ines Montani: https://course.spacy.io/en/\n",
    "- Good documentation, easy to use, and customizable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'en_core_web_md'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01men_core_web_md\u001b[39;00m  \u001b[38;5;66;03m# pre-trained model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      4\u001b[0m nlp \u001b[38;5;241m=\u001b[39m en_core_web_md\u001b[38;5;241m.\u001b[39mload()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'en_core_web_md'"
     ]
    }
   ],
   "source": [
    "import en_core_web_md  # pre-trained model\n",
    "import spacy\n",
    "\n",
    "nlp = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "sample_text = \"\"\"Dolly Parton is a gift to us all. \n",
    "From writing all-time great songs like “Jolene” and “I Will Always Love You”, \n",
    "to great performances in films like 9 to 5, to helping fund a COVID-19 vaccine, \n",
    "she’s given us so much. Now, Netflix bring us Dolly Parton’s Christmas on the Square, \n",
    "an original musical that stars Christine Baranski as a Scrooge-like landowner \n",
    "who threatens to evict an entire town on Christmas Eve to make room for a new mall. \n",
    "Directed and choreographed by the legendary Debbie Allen and counting Jennifer Lewis \n",
    "and Parton herself amongst its cast, Christmas on the Square seems like the perfect movie\n",
    "to save Christmas 2020. 😻 👍🏿\"\"\"\n",
    "\n",
    "# [Adapted from here.](https://thepopbreak.com/2020/11/22/dolly-partons-christmas-on-the-square-review-not-quite-a-christmas-miracle/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Spacy extracts all interesting information from text with this call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's look at part-of-speech tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m([(token, token\u001b[38;5;241m.\u001b[39mpos_) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdoc\u001b[49m][:\u001b[38;5;241m20\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'doc' is not defined"
     ]
    }
   ],
   "source": [
    "print([(token, token.pos_) for token in doc][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Often we want to know **who did what to whom**.\n",
    "- **Named entities** give you this information.  \n",
    "- What are named entities in the text? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Named entities:\\n\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "print(\"\\nORG means: \", spacy.explain(\"ORG\"))\n",
    "print(\"\\nPERSON means: \", spacy.explain(\"PERSON\"))\n",
    "print(\"\\nDATE means: \", spacy.explain(\"DATE\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  An example from a project \n",
    "\n",
    "Goal: Extract and visualize inter-corporate relationships from disclosed annual 10-K reports of public companies. \n",
    "\n",
    "[Source for the text below.](https://www.bbc.com/news/business-39875417)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\n",
    "    \"Heavy hitters, including Microsoft and Google, \"\n",
    "    \"are competing for customers in cloud services with the likes of IBM and Salesforce.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)\n",
    "displacy.render(doc, style=\"ent\")\n",
    "print(\"Named entities:\\n\", [(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you want emoji identification support install [`spacymoji`](https://pypi.org/project/spacymoji/) in the course environment. \n",
    "\n",
    "```\n",
    "pip install spacymoji\n",
    "```\n",
    "\n",
    "After installing `spacymoji`, if it's still complaining about module not found, my guess is that you do not have `pip` installed in your `conda` environment. Go to your course `conda` environment install `pip` and install the `spacymoji` package in the environment using the `pip` you just installed in the current environment. \n",
    "\n",
    "```\n",
    "conda install pip\n",
    "YOUR_MINICONDA_PATH/miniconda3/envs/cpsc330/bin/pip install spacymoji\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from spacymoji import Emoji\n",
    "\n",
    "nlp.add_pipe(\"emoji\", first=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Does the text have any emojis? If yes, extract the description. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(sample_text)\n",
    "doc._.emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple feature engineering for our problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_md\n",
    "import spacy\n",
    "\n",
    "nlp = en_core_web_md.load()\n",
    "from spacymoji import Emoji\n",
    "\n",
    "nlp.add_pipe(\"emoji\", first=True)\n",
    "\n",
    "def get_relative_length(text, TWITTER_ALLOWED_CHARS=280.0):\n",
    "    \"\"\"\n",
    "    Returns the relative length of text.\n",
    "\n",
    "    Parameters:\n",
    "    ------\n",
    "    text: (str)\n",
    "    the input text\n",
    "\n",
    "    Keyword arguments:\n",
    "    ------\n",
    "    TWITTER_ALLOWED_CHARS: (float)\n",
    "    the denominator for finding relative length\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    relative length of text: (float)\n",
    "\n",
    "    \"\"\"\n",
    "    return len(text) / TWITTER_ALLOWED_CHARS\n",
    "\n",
    "\n",
    "def get_length_in_words(text):\n",
    "    \"\"\"\n",
    "    Returns the length of the text in words.\n",
    "\n",
    "    Parameters:\n",
    "    ------\n",
    "    text: (str)\n",
    "    the input text\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    length of tokenized text: (int)\n",
    "\n",
    "    \"\"\"\n",
    "    return len(nltk.word_tokenize(text))\n",
    "\n",
    "\n",
    "def get_sentiment(text):\n",
    "    \"\"\"\n",
    "    Returns the compound score representing the sentiment: -1 (most extreme negative) and +1 (most extreme positive)\n",
    "    The compound score is a normalized score calculated by summing the valence scores of each word in the lexicon.\n",
    "\n",
    "    Parameters:\n",
    "    ------\n",
    "    text: (str)\n",
    "    the input text\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    sentiment of the text: (str)\n",
    "    \"\"\"\n",
    "    scores = sid.polarity_scores(text)\n",
    "    return scores[\"compound\"]\n",
    "\n",
    "def get_avg_word_length(text):\n",
    "    \"\"\"\n",
    "    Returns the average word length of the given text.\n",
    "\n",
    "    Parameters:\n",
    "    text -- (str)\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    return sum(len(word) for word in words) / len(words)\n",
    "\n",
    "\n",
    "def has_emoji(text):\n",
    "    \"\"\"\n",
    "    Returns the average word length of the given text.\n",
    "\n",
    "    Parameters:\n",
    "    text -- (str)\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    return 1 if doc._.has_emoji else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.assign(n_words=train_df[\"OriginalTweet\"].apply(get_length_in_words))\n",
    "train_df = train_df.assign(vader_sentiment=train_df[\"OriginalTweet\"].apply(get_sentiment))\n",
    "train_df = train_df.assign(rel_char_len=train_df[\"OriginalTweet\"].apply(get_relative_length))\n",
    "\n",
    "test_df = test_df.assign(n_words=test_df[\"OriginalTweet\"].apply(get_length_in_words))\n",
    "test_df = test_df.assign(vader_sentiment=test_df[\"OriginalTweet\"].apply(get_sentiment))\n",
    "test_df = test_df.assign(rel_char_len=test_df[\"OriginalTweet\"].apply(get_relative_length))\n",
    "\n",
    "\n",
    "train_df = train_df.assign(\n",
    "    average_word_length=train_df[\"OriginalTweet\"].apply(get_avg_word_length)\n",
    ")\n",
    "test_df = test_df.assign(average_word_length=test_df[\"OriginalTweet\"].apply(get_avg_word_length))\n",
    "\n",
    "# whether all letters are uppercase or not (all_caps)\n",
    "train_df = train_df.assign(\n",
    "    all_caps=train_df[\"OriginalTweet\"].apply(lambda x: 1 if x.isupper() else 0)\n",
    ")\n",
    "test_df = test_df.assign(\n",
    "    all_caps=test_df[\"OriginalTweet\"].apply(lambda x: 1 if x.isupper() else 0)\n",
    ")\n",
    "\n",
    "train_df = train_df.assign(has_emoji=train_df[\"OriginalTweet\"].apply(has_emoji))\n",
    "test_df = test_df.assign(has_emoji=test_df[\"OriginalTweet\"].apply(has_emoji))\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_df['all_caps'] == 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop(columns=['Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['vader_sentiment', \n",
    "                    'rel_char_len', \n",
    "                    'average_word_length']\n",
    "passthrough_features = ['all_caps', 'has_emoji'] \n",
    "text_feature = 'OriginalTweet'\n",
    "drop_features = ['UserName', 'ScreenName', 'Location', 'TweetAt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = make_column_transformer(\n",
    "    (StandardScaler(), numeric_features),\n",
    "    (\"passthrough\", passthrough_features), \n",
    "    (CountVectorizer(stop_words='english'), text_feature),\n",
    "    (\"drop\", drop_features)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(preprocessor, LogisticRegression(max_iter=1000))\n",
    "results[\"LR (more feats)\"] = mean_std_cross_val_scores(\n",
    "    pipe, X_train, y_train, return_train_score=True, scoring=scoring_metrics\n",
    ")\n",
    "pd.DataFrame(results).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_feats = pipe.named_steps['columntransformer'].named_transformers_['countvectorizer'].get_feature_names_out().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_names = numeric_features + passthrough_features + cv_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = pipe.named_steps['logisticregression'].coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    data={\n",
    "        \"features\": feat_names,\n",
    "        \"coefficients\": coefs,\n",
    "    }\n",
    ")\n",
    "df.sort_values('coefficients')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get some improvements with our engineered features! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interim summary \n",
    "\n",
    "- Feature engineering is finding the useful representation of the data that can help us effectively solve our problem. \n",
    "- In the context of text data, if we want to go beyond bag-of-words and incorporate human knowledge in models, we carry out feature engineering. \n",
    "- Some common features include:\n",
    "    - ngram features\n",
    "    - part-of-speech features\n",
    "    - named entity features\n",
    "    - emoticons in text\n",
    "- These are usually extracted from pre-trained models using libraries such as `spaCy`.  \n",
    "- Now a lot of this has moved to deep learning (for text).\n",
    "- But many industries still rely on manual feature engineering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The best features are application-dependent.\n",
    "- It's hard to give general advice. But here are some guidelines. \n",
    "    - Ask the domain experts.\n",
    "    - Go through academic papers in the discipline. \n",
    "    - Often have idea of right discretization/standardization/transformation.\n",
    "    - If no domain expert, cross-validation will help.\n",
    "- If you have lots of data, use deep learning methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<blockquote>\n",
    "    The algorithms we used are very standard for Kagglers ... We spent most of our efforts in feature engineering... <br>\n",
    "- Xavier Conort, on winning the Flight Quest challenge on Kaggle    \n",
    "</blockquote>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Break (5 min)\n",
    "\n",
    "![](../img/eva-coffee.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feature selection: Introduction and motivation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- With so many ways to add new features, we can increase dimensionality of the data. \n",
    "- More features means more complex models, which means increasing the chance of overfitting. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is feature selection?\n",
    "\n",
    "- Find the features\t(columns) $X$ that are important for predicting\t$y$, and remove the features that aren't. \n",
    "\n",
    "- Given $X = \\begin{bmatrix}x_1 & x_2 & \\dots & x_n\\\\  \\\\  \\\\  \\end{bmatrix}$ and $y = \\begin{bmatrix}\\\\  \\\\  \\\\  \\end{bmatrix}$, find the columns $1 \\leq j \\leq n$ in $X$ that are important for predicting $y$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why feature selection? \n",
    "\n",
    "- **Interpretability**: Models are more interpretable with fewer features. If you get the same performance with 10 features instead of 500 features, why not use the model with smaller number of features?     \n",
    "- **Computation**: Models fit/predict faster with fewer columns.\n",
    "- **Data collection**: What type of new data should I collect? It may be cheaper to collect fewer columns.\n",
    "- **Fundamental tradeoff**: Can I reduce overfitting by removing useless features?\n",
    "\n",
    "Feature selection can often result in better performing (less overfit), easier to understand, and faster model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### **How** do we carry out feature selection? \n",
    "\n",
    "- There are a number of ways. \n",
    "- You could use **domain knowledge** to **discard** features. \n",
    "- We are briefly going to look at some **automatic feature selection** methods from `sklearn`: \n",
    "    - Model-based selection \n",
    "    - Recursive feature elimination\n",
    "    - Forward selection \n",
    "- Very **related** to looking at **feature importances**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, cancer.target, random_state=0, test_size=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pipe_lr_all_feats = make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000))\n",
    "pipe_lr_all_feats.fit(X_train, y_train)\n",
    "pd.DataFrame(\n",
    "    cross_validate(pipe_lr_all_feats, X_train, y_train, return_train_score=True)\n",
    ").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model-based selection\n",
    "\n",
    "- Use a supervised machine learning **model to judge the importance** of each feature.\n",
    "- Keep only the most important ones. \n",
    "- Supervised machine learning **model used for feature selection** can be **different** that the one used as the **final estimator**. \n",
    "- Use a model which has some way to calculate feature importances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- To use model-based selection, we use `SelectFromModel` transformer.\n",
    "- It **selects** features which have the **feature importances greater** than the provided **threshold**.\n",
    "- Below I'm using `RandomForestClassifier` for feature selection with threahold \"median\" of feature importances. \n",
    "- Approximately how many features will be selected? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "select_rf = SelectFromModel(\n",
    "    RandomForestClassifier(n_estimators=100, random_state=42), \n",
    "    threshold=\"median\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Can we use KNN to select features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "select_knn = SelectFromModel(\n",
    "    KNeighborsClassifier(), \n",
    "    threshold=\"median\"\n",
    ")\n",
    "\n",
    "pipe_lr_model_based = make_pipeline(\n",
    "    StandardScaler(), select_knn, LogisticRegression(max_iter=1000)\n",
    ")\n",
    "\n",
    "#pd.DataFrame(\n",
    "#    cross_validate(pipe_lr_model_based, X_train, y_train, return_train_score=True)#\n",
    "#).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**No** KNN won't work since it does not report feature importances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What about SVC?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "select_svc = SelectFromModel(\n",
    "    SVC(), threshold=\"median\"\n",
    ")\n",
    "\n",
    "# pipe_lr_model_based = make_pipeline(\n",
    "#     StandardScaler(), select_svc, LogisticRegression(max_iter=1000)\n",
    "# )\n",
    "\n",
    "# pd.DataFrame(\n",
    "#    cross_validate(pipe_lr_model_based, X_train, y_train, return_train_score=True)\n",
    "# ).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Only with a linear kernel but not with RBF kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can put the **feature selection transformer** in a pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pipe_lr_model_based = make_pipeline(\n",
    "    StandardScaler(), select_rf, LogisticRegression(max_iter=1000)\n",
    ")\n",
    "\n",
    "pd.DataFrame(\n",
    "    cross_validate(pipe_lr_model_based, X_train, y_train, return_train_score=True)\n",
    ").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lr_model_based.fit(X_train, y_train)\n",
    "pipe_lr_model_based.named_steps[\"selectfrommodel\"].transform(X_train).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar results with only 15 features instead of 30 features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recursive feature elimination (RFE)\n",
    "\n",
    "- Build a series of models\n",
    "- At each iteration, discard the least important feature according to the model. \n",
    "- Computationally expensive\n",
    "- Basic idea\n",
    "    - fit model\n",
    "    - find least important feature\n",
    "    - remove\n",
    "    - iterate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### RFE algorithm \n",
    "\n",
    "1. Decide $k$, the number of features to select. \n",
    "2. Assign importances to features, e.g. by fitting a model and looking at `coef_` or `feature_importances_`.\n",
    "3. Remove the least important feature.\n",
    "4. Repeat steps 2-3 until only $k$ features are remaining.\n",
    "\n",
    "Note that this is **not** the same as just removing all the less important features in one shot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# create ranking of features\n",
    "rfe = RFE(LogisticRegression(), n_features_to_select=5)\n",
    "rfe.fit(X_train_scaled, y_train)\n",
    "rfe.ranking_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(rfe.support_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"selected features: \", cancer.feature_names[rfe.support_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How do we know what value to pass to `n_features_to_select`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Use `RFECV` which uses cross-validation to select number of features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "rfe_cv = RFECV(LogisticRegression(max_iter=2000), cv=10)\n",
    "rfe_cv.fit(X_train_scaled, y_train)\n",
    "print(rfe_cv.support_)\n",
    "print(cancer.feature_names[rfe_cv.support_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "rfe_pipe = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    RFECV(LogisticRegression(max_iter=2000), cv=10),\n",
    "    RandomForestClassifier(n_estimators=100, random_state=42),\n",
    ")\n",
    "\n",
    "pd.DataFrame(cross_validate(rfe_pipe, X_train, y_train, return_train_score=True)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Slow because there is cross validation within cross validation \n",
    "- Not a big improvement in scores compared to all features on this toy case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### (Optional) Search and score\n",
    "\n",
    "- Define a **scoring function** $f(S)$ that measures the quality of the set of features $S$. \n",
    "- Now **search** for the set of features $S$ with the best score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### General idea of search and score methods \n",
    "\n",
    "- Example: Suppose you have three features: $A, B, C$\n",
    "    - Compute **score** for $S = \\{\\}$\n",
    "    - Compute **score** for $S = \\{A\\}$\n",
    "    - Compute **score** for $S= \\{B\\}$\n",
    "    - Compute **score** for $S = \\{C\\}$\n",
    "    - Compute **score** for $S = \\{A,B\\}$    \n",
    "    - Compute **score** for $S = \\{A,C\\}$\n",
    "    - Compute **score** for $S = \\{B,C\\}$\n",
    "    - Compute **score** for $S = \\{A,B,C\\}$    \n",
    "- Return $S$ with the best score.  \n",
    "- How many distinct combinations we have to try out? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### (Optional) Forward or backward selection \n",
    "\n",
    "- Also called wrapper methods\n",
    "- Shrink or grow feature set by removing or adding one feature at a time \n",
    "- Makes the decision based on whether adding/removing the feature improves the CV score or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../img/forward_selection.png)\n",
    "\n",
    "<!-- <img src='img/forward_selection.png' width=\"1000\" height=\"1000\" /> -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "# pipe_forward = make_pipeline(\n",
    "#     StandardScaler(),\n",
    "#     SequentialFeatureSelector(LogisticRegression(max_iter=1000), \n",
    "#                               direction=\"forward\", \n",
    "#                               n_features_to_select='auto', \n",
    "#                               tol=None),\n",
    "#     RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "# )\n",
    "# pd.DataFrame(\n",
    "#     cross_validate(pipe_forward, X_train, y_train, return_train_score=True)\n",
    "# ).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# pipe_forward = make_pipeline(\n",
    "#     StandardScaler(),\n",
    "#     SequentialFeatureSelector(\n",
    "#         LogisticRegression(max_iter=1000), \n",
    "#                            direction=\"backward\", \n",
    "#                            n_features_to_select=15),\n",
    "#     RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "# )\n",
    "# pd.DataFrame(\n",
    "#     cross_validate(pipe_forward, X_train, y_train, return_train_score=True)\n",
    "# ).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Other ways to search \n",
    "\n",
    "- Stochastic local search\n",
    "    - Inject randomness so that we can explore new parts of the search space\n",
    "    - Simulated annealing\n",
    "    - Genetic algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Warnings about feature selection \n",
    "\n",
    "- A feature's relevance is only defined in the context of other features.\n",
    "    - Adding/removing features can make features relevant/irrelevant.\n",
    "- If features can be predicted from other features, you cannot know which one to pick. \n",
    "- Relevance of features does not have a causal relationship. \n",
    "- Don't be overconfident. \n",
    "    - The methods we have seen probably do not discover the ground truth and how the world really works.\n",
    "    - They simply tell you which features help in predicting $y_i$ for the data and model you have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise 13.1 \n",
    "\n",
    "**Select all of the following statements which are TRUE.**\n",
    "\n",
    "- (A) Simple association-based feature selection approaches do not take into account the interaction between features.\n",
    "- (B) You can carry out feature selection using linear models by pruning the features which have very small weights (i.e., coefficients less than a threshold).\n",
    "- (C) Forward search is guaranteed to find the best feature set.  \n",
    "- (D) The order of features removed given by `rfe.ranking_` is the same as the order of original feature importances given by the model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### (Optional) Problems with feature selection \n",
    "\n",
    "- The term 'relevance' is not clearly defined.\n",
    "- What all things can go wrong with feature selection?\n",
    "- Attribution: From CPSC 340. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Example: Is \"Relevance\" clearly defined?\n",
    "\n",
    "- Consider a supervised classification task of predicting whether someone has particular genetic variation (SNP)\n",
    "\n",
    "<img src='../img/sex_mom_dad.png' width=\"600\" height=\"600\" />\n",
    "\n",
    "- True model: You almost have the same value as your biological mom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Is \"Relevance\" clearly defined?\n",
    "\n",
    "- True model: You almost have the same value for SNP as your biological mom.\n",
    "    - (SNP = biological mom) with very high probability \n",
    "    - (SNP != biological mom) with very low probability \n",
    "    \n",
    "\n",
    "<img src='../img/SNP.png' width=\"400\" height=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Is \"Relevance\" clearly defined?\n",
    "\n",
    "- What if \"mom\" feature is repeated?\n",
    "- Should we pick both? Should we pick one of them because it predicts the other? \n",
    "- Dependence, collinearity for linear models\n",
    "    - If a feature can be predicted from the other, don't know which one to pick. \n",
    "\n",
    "<img src='../img/sex_mom_mom2_dad.png' width=\"600\" height=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Is \"Relevance\" clearly defined?\n",
    "\n",
    "- What if we add (maternal) \"grandma\" feature?\n",
    "- Is it relevant? \n",
    "    - We can predict SNP accurately using this feature\n",
    "- Conditional independence\n",
    "    - But grandma is irrelevant given biological mom feature\n",
    "    - Relevant features may become irrelevant given other features\n",
    "\n",
    "<img src='../img/sex_mom_dad_grandma.png' width=\"600\" height=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Is \"Relevance\" clearly defined?\n",
    "\n",
    "- What if we do not know biological mom feature and we just have grandma feature\n",
    "- It becomes relevant now. \n",
    "    - Without mom feature this is the best we can do. \n",
    "- General problem (\"taco Tuesday\" problem)\n",
    "    - Features can become relevant due to missing information \n",
    "\n",
    "\n",
    "<img src='../img/sex_dad_grandma.png' width=\"600\" height=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Is \"Relevance\" clearly defined?\n",
    "\n",
    "- Are there any relevant features now?\n",
    "- They may have some common maternal ancestor.   \n",
    "- What if mom likes dad because they share SNP? \n",
    "- General problem (Confounding)\n",
    "    - Hidden features can make irrelevant features relevant.\n",
    "\n",
    "<img src='../img/sex_dad.png' width=\"600\" height=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Is \"Relevance\" clearly defined?\n",
    "\n",
    "- Now what if we have \"sibling\" feature? \n",
    "- The feature is relevant in predicting SNP but not the cause of SNP. \n",
    "- General problem (non causality)\n",
    "    - the relevant feature may not be causal \n",
    "\n",
    "<img src='../img/sex_dad_sibling.png' width=\"600\" height=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Is \"Relevance\" clearly defined?\n",
    "\n",
    "- What if you are given \"baby\" feature?\n",
    "- Now the sex feature becomes relevant. \n",
    "    - \"baby\" feature is relevant when sex == F\n",
    "    \n",
    "- General problem (context specific relevance)\n",
    "    - adding a feature can make an irrelevant feature relevant\n",
    "\n",
    "<img src='../img/sex_dad_baby.png' width=\"600\" height=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Warnings about feature selection \n",
    "\n",
    "- A feature is only relevant in the context of other features.\n",
    "    - Adding/removing features can make features relevant/irrelevant.\n",
    "- Confounding factors can make irrelevant features the most relevant.\n",
    "- If features can be predicted from other other features, you cannot know which one to pick. \n",
    "- Relevance for features does not have a causal relationship. \n",
    "\n",
    "- Is feature selection completely hopeless?\n",
    "    - It is messy but we still need to do it. So we try to do our best! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### General advice on finding relevant features\n",
    "\n",
    "- Try forward selection. \n",
    "- Try other feature selection methods (e.g., `RFE`, simulated annealing, genetic algorithms)\n",
    "- Talk to domain experts; they probably have an idea why certain features are relevant.\n",
    "- Don't be overconfident. \n",
    "    - The methods we have seen probably do not discover the ground truth and how the world really works.\n",
    "    - They simply tell you which features help in predicting $y_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Relevant resources \n",
    "- [Genome-wide association study](https://en.wikipedia.org/wiki/Genome-wide_association_study)\n",
    "- [sklearn feature selection](https://scikit-learn.org/stable/modules/feature_selection.html)\n",
    "- [PyData: A Practical Guide to Dimensionality Reduction Techniques](https://www.youtube.com/watch?v=ioXKxulmwVQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:cpsc330]",
   "language": "python",
   "name": "conda-env-cpsc330-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
